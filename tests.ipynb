{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.shapes_ds import generate_shape_dataset_torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current CUDA device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f'Current CUDA device: {device}')\n",
    "    print(f'CUDA device name: {torch.cuda.get_device_name(device)}')\n",
    "else:\n",
    "    print('CUDA is not available on your system.')\n",
    "\n",
    "# Create a tensor on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    # Create a tensor on the GPU\n",
    "    x = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "    print(f'Tensor on GPU: {x}')\n",
    "else:\n",
    "    print('CUDA is not available, cannot create a GPU tensor.')\n",
    "\n",
    "# Perform a simple computation on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    a = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "    b = torch.tensor([4.0, 5.0, 6.0]).cuda()\n",
    "    c = a + b\n",
    "    print(f'Sum on GPU: {c}')\n",
    "else:\n",
    "    print('CUDA is not available, cannot perform GPU computation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([3, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_shape_dataset_torch(img_size=128, num_images=500)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True,drop_last=True)\n",
    "pbar = tqdm(iterable=dataloader)\n",
    "for batch in pbar:\n",
    "    batch = batch[0]\n",
    "    print(batch.shape)\n",
    "    for i in range(batch.shape[0]):\n",
    "        print(batch[i].shape)\n",
    "        image = batch[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        #print(image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8510, 0.8510, 0.8510],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.9333, 0.9255, 0.8980],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9961, 0.9922, 0.9176]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8510, 0.8510, 0.8510],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.9333, 0.9255, 0.8980],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9961, 0.9922, 0.9176]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8510, 0.8510, 0.8510],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.9333, 0.9255, 0.8980],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9961, 0.9922, 0.9176]]]]]), tensor([], size=(1, 0)), tensor([], size=(1, 0)), tensor([], size=(1, 0)), tensor([], size=(1, 0))]\n",
      "torch.Size([1, 1, 3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets.get_dataset import get_image_dataset\n",
    "\n",
    "dataset = get_image_dataset(ds='calvin', root='/media/tim/E/datasets/task_D_D', image_size=128, mode='train', seq_len=1)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True,drop_last=True)\n",
    "pbar = tqdm(iterable=dataloader)\n",
    "for batch in pbar:\n",
    "    print(batch)\n",
    "    print(batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/292000 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 128, 128])\n",
      "[tensor([[[[[0.3843, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4784, 0.4745, 0.4745,  ..., 0.6118, 0.6078, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4784, 0.4784,  ..., 0.6078, 0.6039, 0.6039]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4706, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3765, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4667, 0.4627, 0.4627,  ..., 0.5804, 0.5765, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5725, 0.5725]]]]]), tensor([], size=(1, 0)), tensor([], size=(1, 0)), tensor([], size=(1, 0)), tensor([], size=(1, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets.get_dataset import get_image_dataset\n",
    "\n",
    "dataset = get_image_dataset(ds='obj3d128', root='/media/tim/D/OBJ3D', image_size=128, mode='train', seq_len=1)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True,drop_last=True)\n",
    "pbar = tqdm(iterable=dataloader)\n",
    "for batch in pbar:\n",
    "    print(batch[0].shape)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 3, 128, 128])\n",
      "[tensor([[[[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3843, 0.3882],\n",
      "           [0.3882, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4784, 0.4745, 0.4784,  ..., 0.6196, 0.6118, 0.6118],\n",
      "           [0.4784, 0.4824, 0.4784,  ..., 0.6157, 0.6157, 0.6157],\n",
      "           [0.4745, 0.4745, 0.4824,  ..., 0.6196, 0.6157, 0.6118]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5961],\n",
      "           [0.4706, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5961],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.5961, 0.5961, 0.5922]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5765],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5804, 0.5765],\n",
      "           [0.4588, 0.4627, 0.4667,  ..., 0.5765, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3843, 0.3882],\n",
      "           [0.3882, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4784, 0.4745, 0.4784,  ..., 0.6196, 0.6118, 0.6118],\n",
      "           [0.4784, 0.4824, 0.4784,  ..., 0.6157, 0.6157, 0.6157],\n",
      "           [0.4745, 0.4784, 0.4824,  ..., 0.6196, 0.6157, 0.6118]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5961],\n",
      "           [0.4706, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5961],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.6000, 0.5961, 0.5922]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5765],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5765],\n",
      "           [0.4588, 0.4627, 0.4667,  ..., 0.5765, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3843, 0.3882],\n",
      "           [0.3882, 0.3882, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6196, 0.6157, 0.6118],\n",
      "           [0.4784, 0.4824, 0.4784,  ..., 0.6196, 0.6157, 0.6157],\n",
      "           [0.4745, 0.4784, 0.4824,  ..., 0.6196, 0.6157, 0.6118]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5961],\n",
      "           [0.4706, 0.4745, 0.4745,  ..., 0.6000, 0.5961, 0.5961],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.6000, 0.5961, 0.5922]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5765],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5765],\n",
      "           [0.4588, 0.4627, 0.4667,  ..., 0.5804, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4784, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3843, 0.3804,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5922, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4706, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5725, 0.5686]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4784, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3843, 0.3804,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5922, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4706, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5725, 0.5686]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3922,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4784, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3843, 0.3804,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5922, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4706, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5725, 0.5686]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6078, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6078, 0.6039]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4706, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4745, 0.4784, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4706, 0.4745, 0.4745,  ..., 0.6000, 0.6000, 0.5961]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3804, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3843,  ..., 0.3765, 0.3882, 0.3843],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5765, 0.5765, 0.5765],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6078, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6118, 0.6078, 0.6078],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6078, 0.6039]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4706, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4745, 0.4784, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4706, 0.4706, 0.4745,  ..., 0.6000, 0.6000, 0.5961]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3804, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3843,  ..., 0.3765, 0.3882, 0.3843],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5804, 0.5765, 0.5765],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3882, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6078, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6118, 0.6078, 0.6078],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6078, 0.6039]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3922,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3961, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4706, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6039, 0.6000, 0.5961],\n",
      "           [0.4706, 0.4745, 0.4745,  ..., 0.6000, 0.6000, 0.5961]],\n",
      "\n",
      "          [[0.3765, 0.3843, 0.3882,  ..., 0.3843, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3804, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3843,  ..., 0.3765, 0.3882, 0.3843],\n",
      "           ...,\n",
      "           [0.4627, 0.4588, 0.4627,  ..., 0.5804, 0.5765, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5804, 0.5765, 0.5765],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5765, 0.5765, 0.5725]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3882, 0.3843,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3804, 0.3882, 0.3843,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4627, 0.4627, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5725, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5725, 0.5725, 0.5686]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3843, 0.3843],\n",
      "           [0.3843, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4667, 0.4706,  ..., 0.6000, 0.5961, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3882, 0.3843,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3804, 0.3882, 0.3843,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4667, 0.4627, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5725, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5725, 0.5725, 0.5686]]],\n",
      "\n",
      "\n",
      "         [[[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3882, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.6039, 0.6039],\n",
      "           [0.4784, 0.4784, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
      "           [0.4745, 0.4745, 0.4784,  ..., 0.6078, 0.6039, 0.6000]],\n",
      "\n",
      "          [[0.3804, 0.3882, 0.3882,  ..., 0.3804, 0.3843, 0.3843],\n",
      "           [0.3843, 0.3843, 0.3843,  ..., 0.3843, 0.3804, 0.3882],\n",
      "           [0.3843, 0.3922, 0.3882,  ..., 0.3804, 0.3882, 0.3882],\n",
      "           ...,\n",
      "           [0.4706, 0.4667, 0.4706,  ..., 0.6000, 0.5922, 0.5922],\n",
      "           [0.4745, 0.4745, 0.4745,  ..., 0.5961, 0.5961, 0.5922],\n",
      "           [0.4667, 0.4706, 0.4745,  ..., 0.5961, 0.5922, 0.5922]],\n",
      "\n",
      "          [[0.3765, 0.3882, 0.3843,  ..., 0.3804, 0.3804, 0.3843],\n",
      "           [0.3843, 0.3804, 0.3804,  ..., 0.3804, 0.3765, 0.3843],\n",
      "           [0.3804, 0.3882, 0.3843,  ..., 0.3804, 0.3843, 0.3882],\n",
      "           ...,\n",
      "           [0.4667, 0.4627, 0.4627,  ..., 0.5804, 0.5725, 0.5725],\n",
      "           [0.4667, 0.4667, 0.4667,  ..., 0.5725, 0.5765, 0.5725],\n",
      "           [0.4627, 0.4627, 0.4667,  ..., 0.5725, 0.5725, 0.5686]]]]]), tensor([], size=(2, 0)), tensor([], size=(2, 0)), tensor([], size=(2, 0)), tensor([], size=(2, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets.get_dataset import get_video_dataset\n",
    "\n",
    "dataset = get_video_dataset(ds='obj3d128', root='/media/tim/D/OBJ3D', image_size=128, mode='train', seq_len=100)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2, num_workers=0, pin_memory=True,drop_last=True)\n",
    "pbar = tqdm(iterable=dataloader)\n",
    "for batch in pbar:\n",
    "    print(batch[0].shape)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 3, 128, 128])\n",
      "[tensor([[[[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8667, 0.8627, 0.8510],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.9647, 0.9529, 0.8824]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]],\n",
      "\n",
      "\n",
      "         [[[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]],\n",
      "\n",
      "          [[0.8667, 0.8667, 0.8667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.9725, 0.9725, 0.9686],\n",
      "           [0.8667, 0.8667, 0.8667,  ..., 0.8863, 0.8824, 0.8784],\n",
      "           ...,\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6941, 0.6902, 0.6902,  ..., 0.8471, 0.8471, 0.8471],\n",
      "           [0.6980, 0.6941, 0.6941,  ..., 0.8471, 0.8471, 0.8471]]]]]), tensor([], size=(2, 0)), tensor([], size=(2, 0)), tensor([], size=(2, 0)), tensor([], size=(2, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets.get_dataset import get_video_dataset\n",
    "\n",
    "dataset = get_video_dataset(ds='calvin', root='/media/tim/E/datasets/task_D_D', image_size=128, mode='val', seq_len=32)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2, num_workers=0, pin_memory=True,drop_last=True)\n",
    "pbar = tqdm(iterable=dataloader)\n",
    "for batch in pbar:\n",
    "    print(batch[0].shape)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/tim/D/ddlp/tests.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39m# for batch in dataloader:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39m#print(batch[0].shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39m#print(batch)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m     \u001b[39m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         \u001b[39m#model_output = model(x, x_prior=x_prior)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/tim/D/ddlp/tests.ipynb#X10sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m evaluate_validation_elbo_dyn(model, config, \u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m/media/tim/D/ddlp/eval/eval_model.py:198\u001b[0m, in \u001b[0;36mevaluate_validation_elbo_dyn\u001b[0;34m(model, config, epoch, batch_size, recon_loss_type, device, save_image, fig_dir, topk, recon_loss_func, beta_rec, beta_kl, beta_dyn, iou_thresh, beta_dyn_rec, kl_balance, accelerator, timestep_horizon, animation_horizon)\u001b[0m\n\u001b[1;32m    196\u001b[0m x_prior \u001b[39m=\u001b[39m x\n\u001b[1;32m    197\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 198\u001b[0m     model_output \u001b[39m=\u001b[39m model(x, x_prior\u001b[39m=\u001b[39;49mx_prior)\n\u001b[1;32m    199\u001b[0m     \u001b[39m# calc elbo\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcalc_elbo(x, model_output, beta_kl\u001b[39m=\u001b[39mbeta_kl,\n\u001b[1;32m    201\u001b[0m                              beta_dyn\u001b[39m=\u001b[39mbeta_dyn, beta_rec\u001b[39m=\u001b[39mbeta_rec, kl_balance\u001b[39m=\u001b[39mkl_balance,\n\u001b[1;32m    202\u001b[0m                              recon_loss_type\u001b[39m=\u001b[39mrecon_loss_type, recon_loss_func\u001b[39m=\u001b[39mrecon_loss_func,\n\u001b[1;32m    203\u001b[0m                              beta_dyn_rec\u001b[39m=\u001b[39mbeta_dyn_rec)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tim/D/ddlp/models.py:2458\u001b[0m, in \u001b[0;36mObjectDynamicsDLP.forward\u001b[0;34m(self, x, deterministic, bg_masks_from_fg, x_prior, warmup, noisy, forward_dyn, train_enc_prior, num_static_frames)\u001b[0m\n\u001b[1;32m   2455\u001b[0m x_in \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])  \u001b[39m# [bs * T, ...]\u001b[39;00m\n\u001b[1;32m   2456\u001b[0m \u001b[39mif\u001b[39;00m forward_dyn:\n\u001b[1;32m   2457\u001b[0m     \u001b[39m# tracking\u001b[39;00m\n\u001b[0;32m-> 2458\u001b[0m     fg_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfg_sequential_opt(x, deterministic\u001b[39m=\u001b[39;49mdeterministic, x_prior\u001b[39m=\u001b[39;49mx_prior, warmup\u001b[39m=\u001b[39;49mwarmup,\n\u001b[1;32m   2459\u001b[0m                                      noisy\u001b[39m=\u001b[39;49mnoisy,\n\u001b[1;32m   2460\u001b[0m                                      reshape\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, train_prior\u001b[39m=\u001b[39;49mtrain_enc_prior,\n\u001b[1;32m   2461\u001b[0m                                      num_static_frames\u001b[39m=\u001b[39;49mnum_static_frames)\n\u001b[1;32m   2462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2463\u001b[0m     \u001b[39m# static\u001b[39;00m\n\u001b[1;32m   2464\u001b[0m     x_prior \u001b[39m=\u001b[39m x_in\n",
      "File \u001b[0;32m/media/tim/D/ddlp/models.py:2280\u001b[0m, in \u001b[0;36mObjectDynamicsDLP.fg_sequential_opt\u001b[0;34m(self, x, deterministic, x_prior, warmup, noisy, reshape, train_prior, num_static_frames)\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[39m# for the first time step, standard encoding\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m filtering_heuristic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiltering_heuristic\n\u001b[0;32m-> 2280\u001b[0m kp_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfg_module\u001b[39m.\u001b[39;49mencode_prior(x[:, :num_static_frames]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m:]),\n\u001b[1;32m   2281\u001b[0m                                    x_prior\u001b[39m=\u001b[39;49mx_prior[:, :num_static_frames]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49mx_prior\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m:]),\n\u001b[1;32m   2282\u001b[0m                                    filtering_heuristic\u001b[39m=\u001b[39;49mfiltering_heuristic)\n\u001b[1;32m   2283\u001b[0m kp_p \u001b[39m=\u001b[39m kp_p\u001b[39m.\u001b[39mreshape(batch_size, num_static_frames, \u001b[39m*\u001b[39mkp_p\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])  \u001b[39m# [bs, n_stat_frames, n_kp_p, 2]\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m kp_p \u001b[39m=\u001b[39m kp_p \u001b[39mif\u001b[39;00m train_prior \u001b[39melse\u001b[39;00m (\u001b[39m0.0\u001b[39m \u001b[39m*\u001b[39m kp_p \u001b[39m+\u001b[39m kp_p\u001b[39m.\u001b[39mdetach())  \u001b[39m# 0.0 * kp_p for distributed training\u001b[39;00m\n",
      "File \u001b[0;32m/media/tim/D/ddlp/models.py:295\u001b[0m, in \u001b[0;36mFgDLP.encode_prior\u001b[0;34m(self, x, x_prior, filtering_heuristic, k)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m x_prior \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     x_prior \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 295\u001b[0m kp_p, var_kp_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior(x_prior, global_kp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    296\u001b[0m kp_p \u001b[39m=\u001b[39m kp_p\u001b[39m.\u001b[39mview(x_prior\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# [batch_size, n_kp_total, 2]\u001b[39;00m\n\u001b[1;32m    297\u001b[0m var_kp_p \u001b[39m=\u001b[39m var_kp_p\u001b[39m.\u001b[39mview(x_prior\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], kp_p\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# [batch_size, n_kp_total, 3]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tim/D/ddlp/modules/modules.py:464\u001b[0m, in \u001b[0;36mVariationalKeyPointPatchEncoder.forward\u001b[0;34m(self, x, global_kp)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, global_kp\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x, global_kp)\n",
      "File \u001b[0;32m/media/tim/D/ddlp/modules/modules.py:441\u001b[0m, in \u001b[0;36mVariationalKeyPointPatchEncoder.encode\u001b[0;34m(self, x, global_kp)\u001b[0m\n\u001b[1;32m    439\u001b[0m x_patches \u001b[39m=\u001b[39m x_patches\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)  \u001b[39m# [batch_size, num_patches, cdim, patch_size, patch_size]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m x_patches \u001b[39m=\u001b[39m x_patches\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cdim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatcher\u001b[39m.\u001b[39mpatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatcher\u001b[39m.\u001b[39mpatch_size)\n\u001b[0;32m--> 441\u001b[0m _, z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(x_patches)  \u001b[39m# [batch_size*num_patches, n_kp, features_dim, features_dim]\u001b[39;00m\n\u001b[1;32m    442\u001b[0m mu_kp, var_kp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssm(z, probs\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, variance\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# [batch_size * num_patches, n_kp, 2]\u001b[39;00m\n\u001b[1;32m    443\u001b[0m mu_kp \u001b[39m=\u001b[39m mu_kp\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_kp, \u001b[39m2\u001b[39m)  \u001b[39m# [batch_size, num_patches, n_kp, 2]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tim/D/ddlp/modules/modules.py:154\u001b[0m, in \u001b[0;36mKeyPointCNNOriginal.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 154\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x)\n\u001b[1;32m    155\u001b[0m     \u001b[39m# heatmap\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     hm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeymap(y)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tim/D/ddlp/modules/modules.py:107\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 107\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models import ObjectDynamicsDLP\n",
    "from utils.loss_functions import calc_reconstruction_loss, VGGDistance\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "# datasets\n",
    "from datasets.get_dataset import get_video_dataset, get_image_dataset\n",
    "# util functions\n",
    "from utils.util_func import get_config, plot_keypoints_on_image_batch, animate_trajectories, \\\n",
    "    plot_bb_on_image_batch_from_z_scale_nms, plot_bb_on_image_batch_from_masks_nms\n",
    "from eval.eval_model import evaluate_validation_elbo_dyn\n",
    "\n",
    "\n",
    "config_path = './checkpoints/calvin_ddlp/hparams.json'\n",
    "config = get_config(config_path)\n",
    "\n",
    "hparams = config  # to save a copy of the hyper-parameters\n",
    "# data and general\n",
    "ds = config['ds']\n",
    "ch = config['ch']  # image channels\n",
    "image_size = config['image_size']\n",
    "root = config['root']  # dataset root\n",
    "animation_horizon = config['animation_horizon']\n",
    "batch_size = config['batch_size']\n",
    "lr = config['lr']\n",
    "num_epochs = config['num_epochs']\n",
    "topk = min(config['topk'], config['n_kp_enc'])  # top-k particles to plot\n",
    "eval_epoch_freq = config['eval_epoch_freq']\n",
    "weight_decay = config['weight_decay']\n",
    "iou_thresh = config['iou_thresh']  # threshold for NMS for plotting bounding boxes\n",
    "run_prefix = config['run_prefix']\n",
    "load_model = config['load_model']\n",
    "pretrained_path = config['pretrained_path']  # path of pretrained model to load, if None, train from scratch\n",
    "adam_betas = config['adam_betas']\n",
    "adam_eps = config['adam_eps']\n",
    "scheduler_gamma = config['scheduler_gamma']\n",
    "eval_im_metrics = config['eval_im_metrics']\n",
    "cond_steps = config['cond_steps']  # conditional frames for the dynamics module during inference\n",
    "device = config['device']\n",
    "if 'cuda' in device:\n",
    "    device = torch.device(f'{device}' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# model\n",
    "timestep_horizon = config['timestep_horizon']\n",
    "kp_range = config['kp_range']\n",
    "kp_activation = config['kp_activation']\n",
    "enc_channels = config['enc_channels']\n",
    "prior_channels = config['prior_channels']\n",
    "pad_mode = config['pad_mode']\n",
    "n_kp = config['n_kp']  # kp per patch in prior, best to leave at 1\n",
    "n_kp_prior = config['n_kp_prior']  # number of prior kp to filter for the kl\n",
    "n_kp_enc = config['n_kp_enc']  # total posterior kp\n",
    "patch_size = config['patch_size']  # prior patch size\n",
    "anchor_s = config['anchor_s']  # posterior patch/glimpse ratio of image size\n",
    "learned_feature_dim = config['learned_feature_dim']\n",
    "dropout = config['dropout']\n",
    "use_resblock = config['use_resblock']\n",
    "use_correlation_heatmaps = config['use_correlation_heatmaps']  # use heatmaps for tracking\n",
    "enable_enc_attn = config['enable_enc_attn']  # enable attention between patches in the particle encoder\n",
    "filtering_heuristic = config[\"filtering_heuristic\"]  # filtering heuristic to filter prior keypoints\n",
    "\n",
    "# optimization\n",
    "warmup_epoch = config['warmup_epoch']\n",
    "recon_loss_type = config['recon_loss_type']\n",
    "beta_kl = config['beta_kl']\n",
    "beta_dyn = config['beta_dyn']\n",
    "beta_rec = config['beta_rec']\n",
    "beta_dyn_rec = config['beta_dyn_rec']\n",
    "kl_balance = config['kl_balance']  # balance between visual features and the other particle attributes\n",
    "num_static_frames = config['num_static_frames']  # frames for which kl is calculated w.r.t constant prior params\n",
    "train_enc_prior = config['train_enc_prior']\n",
    "\n",
    "# priors\n",
    "sigma = config['sigma']  # std for constant kp prior, leave at 1 for deterministic chamfer-kl\n",
    "scale_std = config['scale_std']\n",
    "offset_std = config['offset_std']\n",
    "obj_on_alpha = config['obj_on_alpha']  # transparency beta distribution \"a\"\n",
    "obj_on_beta = config['obj_on_beta']  # transparency beta distribution \"b\"\n",
    "\n",
    "# transformer - PINT\n",
    "pint_layers = config['pint_layers']\n",
    "pint_heads = config['pint_heads']\n",
    "pint_dim = config['pint_dim']\n",
    "predict_delta = config['predict_delta']  # dynamics module predicts the delta from previous step\n",
    "start_epoch = config['start_dyn_epoch']\n",
    "\n",
    "# load data\n",
    "dataset = get_video_dataset(ds, root, seq_len=timestep_horizon + 1, mode='val', image_size=image_size)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True,\n",
    "                        drop_last=True)\n",
    "# model\n",
    "model = ObjectDynamicsDLP(cdim=ch, enc_channels=enc_channels, prior_channels=prior_channels,\n",
    "                            image_size=image_size, n_kp=n_kp, learned_feature_dim=learned_feature_dim,\n",
    "                            pad_mode=pad_mode, sigma=sigma,\n",
    "                            dropout=dropout, patch_size=patch_size, n_kp_enc=n_kp_enc,\n",
    "                            n_kp_prior=n_kp_prior, kp_range=kp_range, kp_activation=kp_activation,\n",
    "                            anchor_s=anchor_s, use_resblock=use_resblock,\n",
    "                            timestep_horizon=timestep_horizon, predict_delta=predict_delta,\n",
    "                            scale_std=scale_std, offset_std=offset_std, obj_on_alpha=obj_on_alpha,\n",
    "                            obj_on_beta=obj_on_beta, pint_layers=pint_layers, pint_heads=pint_heads,\n",
    "                            pint_dim=pint_dim, use_correlation_heatmaps=use_correlation_heatmaps,\n",
    "                            enable_enc_attn=enable_enc_attn, filtering_heuristic=filtering_heuristic).to(device)\n",
    "\n",
    "path_to_model_ckpt = './checkpoints/calvin_ddlp/calvin_ddlp.pth'\n",
    "model.load_state_dict(torch.load(path_to_model_ckpt, map_location=device), strict=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataset = get_video_dataset(ds, root, seq_len=timestep_horizon + 1, mode='valid', image_size=image_size)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4, drop_last=False)\n",
    "# for batch in dataloader:\n",
    "    #print(batch[0].shape)\n",
    "    #print(batch)\n",
    "    #x = batch[0][:, :timestep_horizon + 1].to(device)\n",
    "    #print('hAHSDAH: ', x.shape)\n",
    "    #x_prior = x\n",
    "    #with torch.no_grad():\n",
    "        #model_output = model(x, x_prior=x_prior)\n",
    "\n",
    "evaluate_validation_elbo_dyn(model, config, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
